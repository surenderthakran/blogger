<h2>What is a Neural Network?</h2>
<p>Neural Networks, more appropriately referred to as an 'Artificial Neural
  Networks' (ANN), are information processing systems that are inspired by the
  biological neural networks such as the brain.
<p>In its simplest form a brain is a huge collection of neurons. Each neuron
  takes electrical and chemical signals as inputs through its many dentrites and
  transmits its output signals through its axon (in a more specialized context,
  there are exceptions to this behavior). Axons make contact with other neurons
  at specialized junctions called synapses.</p>
<div class="image">
  <img alt="image illustrating a biological neuron" src="/images/{{ url }}/biological-neuron.png" width="500" />
  <a href="https://en.wikipedia.org/wiki/Neuron" target="_blank">Image source</a>
</div>
<p>Taking inspiration from the brain, an artificial neural network is also a
  collection of connected units, also called neurons. The connection between the
  neurons can carry signals between them. Each connection carries a real number
  value which determines the weightage/strength of the signal.</p>
<h3>Topology</h3>
<h2>Learning History</h2>
<h2>Perceptrons vs Neurons</h2>
<p>Perceptron. The simplest and oldest model of Neuron, as we know it. Takes
  some inputs, sums them up, applies activation function and passes them to
  output layer.</p>
<h3>Activation Functions</h3>
<h2>Multi Layer Perceptrons (MLP)</h2>
<h3>Back Propagation Algorithm</h3>
<p>Backpropagation, short for "backward propagation of errors," is an algorithm
  for supervised learning of artificial neural networks using gradient descent.
  Given an artificial neural network and an error function, the method
  calculates the gradient of the error function with respect to the neural
  network's weights.</p>
<h2>Neural Network Implementation</h2>
<h3>Feed Forward</h3>
<h3>Back propagation</h3>
<h2>Example</h2>
