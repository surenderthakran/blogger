<h2>What is a Neural Network?</h2>
<p>Neural Networks, more appropriately referred to as an 'Artificial Neural
  Networks' (ANN), are information processing systems that are inspired by the
  biological neural networks such as the brain.
<p>In its simplest form a brain is a huge collection of neurons. Each neuron
  takes electrical and chemical signals as inputs through its many dentrites and
  transmits its output signals through its axon (in a more specialized context,
  there are exceptions to this behavior). Axons make contact with other neurons
  at specialized junctions called synapses.</p>
<div class="image">
  <img alt="image illustrating a biological neuron" src="/images/{{ url }}/biological-neuron.png" width="500" />
  <a href="https://en.wikipedia.org/wiki/Neuron" target="_blank">Image source</a>
</div>
<p>Taking inspiration from the brain, an artificial neural network is a
  collection of connected units, also called neurons. The connection between the
  neurons can carry signals between them. Each connection carries a real number
  value which determines the weightage/strength of the signal.</p>
<p>They are <strong>universal function approximators</strong> who at there very essence
  approximate a function/relation between the input data and the output
  data.<p>
<h3>Topology Of an Artificial Neural Network</h3>
<p>The simplest form of neural network is a single layer feed forward neural network.</p>
<h2>Learning History</h2>
<h2>Perceptrons vs Neurons</h2>
<p>Perceptron. The simplest and oldest model of Neuron, as we know it. Takes
  some inputs, sums them up, applies activation function and passes them to
  output layer.</p>
<h3>Activation Functions</h3>
<p>Sigmoid, ReLU etc are non-linear activation functions.</p>
<h2>Multi Layer Perceptrons (MLP)</h2>
<h3>Back Propagation Algorithm</h3>
<p>Backpropagation, short for "backward propagation of errors," is an algorithm
  for supervised learning of artificial neural networks using gradient descent.
  Given an artificial neural network and an error function, the method
  calculates the gradient of the error function with respect to the neural
  network's weights.</p>
<p>ReLU helps in solving the vanishing gradient problem.</p>
<h2>Neural Network Implementation</h2>
<h3>Feed Forward</h3>
<h3>Back propagation</h3>
<h2>Example</h2>
